{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "from osgeo import osr\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(devices=physical_devices[0], device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义matries 计算r2\n",
    "def R2(y_true, y_pred):\n",
    "    sst = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    ssr = K.sum(K.square(y_pred - y_true))\n",
    "    R2 = 1 - ssr/sst\n",
    "    return R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Array2Image(data,lats,lons):\n",
    "  # get the unique coordinates\n",
    "  uniqueLats = np.unique(lats)\n",
    "  uniqueLons = np.unique(lons)\n",
    " \n",
    "  # get number of columns and rows from coordinates\n",
    "  ncols = len(uniqueLons)\n",
    "  nrows = len(uniqueLats)\n",
    " \n",
    "  # determine pixelsizes\n",
    "  ys = uniqueLats[1] - uniqueLats[0]  \n",
    "  xs = uniqueLons[1] - uniqueLons[0]\n",
    " \n",
    "  # create an array with dimensions of image\n",
    "  # 这里把缺失值设置成了0，之后需要修改\n",
    "  arr = np.zeros([nrows, ncols], np.float32) #-9999\n",
    "  arr.fill(np.nan)\n",
    " \n",
    "  # fill the array with values\n",
    "  for counter in range(0,len(data),1):\n",
    "    index_lon = np.where(uniqueLons == lons[counter])[0][0]\n",
    "    index_lat = np.where(uniqueLats == lats[counter])[0][0]\n",
    "    arr[len(uniqueLats)-1-index_lat,index_lon] = data[counter]\n",
    "  return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(df:pd.DataFrame):\n",
    "    allData = []\n",
    "    for j in range(df.shape[1]):\n",
    "        data = []\n",
    "        for i in range(df.shape[0]):\n",
    "            #print(df.iloc[i,j])\n",
    "            data_i = gdal.Open(df.iloc[i,j])\n",
    "            band_i = data_i.GetRasterBand(1)\n",
    "            array_i = band_i.ReadAsArray()\n",
    "            #array_i[array_i == noData] = np.nan\n",
    "            data.append(np.reshape(array_i,(-1)))\n",
    "        allData.append(data)\n",
    "    allData = np.array(allData,dtype = np.float64).squeeze()\n",
    "    return pd.DataFrame(allData.T,columns=df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "templete  = gdal.Open('F:/SGYL/SM_Downscaling2/Lon.tif')\n",
    "def convert2Tif(data,outputPath):\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    tif = driver.Create(outputPath,data.shape[1],data.shape[0],1,gdal.GDT_Int16)\n",
    "    #geotransform = (73.49117339381806,0.008983152841195215,0.0,39.831299697859585,0.0,-0.008983152841195215)\n",
    "    tif.SetGeoTransform(templete.GetGeoTransform())\n",
    "    tif.SetProjection(templete.GetProjection())\n",
    "    tif.GetRasterBand(1).WriteArray(data)\n",
    "    #tif.GetRasterBand(1).SetNoDataValue()\n",
    "    tif.FlushCache()\n",
    "    del tif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.4e+38\n"
     ]
    }
   ],
   "source": [
    "noData_i = gdal.Open('F:/SGYL/SM_results_data/DATA/data_2020/LST_Diff/LST_Diff_2020_1.tif')\n",
    "noData_band = noData_i.GetRasterBand(1)\n",
    "noData_arr = noData_band.ReadAsArray()\n",
    "noData = np.min(noData_arr)\n",
    "print(noData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003 ************************************************\n",
      "load model\n",
      "D:/SGYL/SM_results_data/check_points/Full_ResNet/Full_ResNet_2003_best.hdf5\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "load training data\n",
      "['NDVI', 'EVI', 'LST', 'LST_Diff', 'Pre', 'SWCI', 'VSDI', 'SIWSI', 'ET', 'TWI', 'DEM', 'Aspect', 'Slope', 'Clay', 'Sand', 'Silt', 'Lon', 'Lat', 'DOY']\n",
      "prepare file path\n",
      "start predict ************************************************\n",
      "0 : save the 1 tif successfully for the year 2003 \n",
      "1 : save the 2 tif successfully for the year 2003 \n",
      "2 : save the 3 tif successfully for the year 2003 \n",
      "3 : save the 4 tif successfully for the year 2003 \n",
      "4 : save the 5 tif successfully for the year 2003 \n",
      "5 : save the 6 tif successfully for the year 2003 \n",
      "6 : save the 7 tif successfully for the year 2003 \n",
      "7 : save the 8 tif successfully for the year 2003 \n",
      "8 : save the 9 tif successfully for the year 2003 \n",
      "9 : save the 10 tif successfully for the year 2003 \n",
      "10 : save the 11 tif successfully for the year 2003 \n",
      "11 : save the 12 tif successfully for the year 2003 \n",
      "12 : save the 13 tif successfully for the year 2003 \n",
      "13 : save the 14 tif successfully for the year 2003 \n",
      "14 : save the 15 tif successfully for the year 2003 \n",
      "15 : save the 16 tif successfully for the year 2003 \n",
      "16 : save the 17 tif successfully for the year 2003 \n",
      "17 : save the 18 tif successfully for the year 2003 \n",
      "18 : save the 19 tif successfully for the year 2003 \n",
      "19 : save the 20 tif successfully for the year 2003 \n",
      "20 : save the 21 tif successfully for the year 2003 \n",
      "21 : save the 22 tif successfully for the year 2003 \n",
      "22 : save the 23 tif successfully for the year 2003 \n",
      "23 : save the 24 tif successfully for the year 2003 \n",
      "24 : save the 25 tif successfully for the year 2003 \n",
      "25 : save the 26 tif successfully for the year 2003 \n",
      "26 : save the 27 tif successfully for the year 2003 \n",
      "27 : save the 28 tif successfully for the year 2003 \n",
      "28 : save the 29 tif successfully for the year 2003 \n",
      "29 : save the 30 tif successfully for the year 2003 \n",
      "30 : save the 31 tif successfully for the year 2003 \n",
      "31 : save the 32 tif successfully for the year 2003 \n",
      "32 : save the 33 tif successfully for the year 2003 \n",
      "33 : save the 34 tif successfully for the year 2003 \n",
      "34 : save the 35 tif successfully for the year 2003 \n",
      "35 : save the 36 tif successfully for the year 2003 \n",
      "36 : save the 37 tif successfully for the year 2003 \n",
      "37 : save the 38 tif successfully for the year 2003 \n",
      "38 : save the 39 tif successfully for the year 2003 \n",
      "39 : save the 40 tif successfully for the year 2003 \n",
      "40 : save the 41 tif successfully for the year 2003 \n",
      "41 : save the 42 tif successfully for the year 2003 \n",
      "42 : save the 43 tif successfully for the year 2003 \n",
      "43 : save the 44 tif successfully for the year 2003 \n",
      "44 : save the 45 tif successfully for the year 2003 \n",
      "45 : save the 46 tif successfully for the year 2003 \n",
      "46 : save the 47 tif successfully for the year 2003 \n",
      "47 : save the 48 tif successfully for the year 2003 \n",
      "48 : save the 49 tif successfully for the year 2003 \n",
      "49 : save the 50 tif successfully for the year 2003 \n",
      "50 : save the 51 tif successfully for the year 2003 \n",
      "51 : save the 52 tif successfully for the year 2003 \n",
      "52 : save the 53 tif successfully for the year 2003 \n",
      "53 : save the 54 tif successfully for the year 2003 \n",
      "54 : save the 55 tif successfully for the year 2003 \n",
      "55 : save the 56 tif successfully for the year 2003 \n",
      "56 : save the 57 tif successfully for the year 2003 \n",
      "57 : save the 58 tif successfully for the year 2003 \n",
      "58 : save the 59 tif successfully for the year 2003 \n",
      "59 : save the 60 tif successfully for the year 2003 \n",
      "60 : save the 61 tif successfully for the year 2003 \n",
      "61 : save the 62 tif successfully for the year 2003 \n",
      "62 : save the 63 tif successfully for the year 2003 \n",
      "63 : save the 64 tif successfully for the year 2003 \n",
      "64 : save the 65 tif successfully for the year 2003 \n",
      "65 : save the 66 tif successfully for the year 2003 \n",
      "66 : save the 67 tif successfully for the year 2003 \n",
      "67 : save the 68 tif successfully for the year 2003 \n",
      "68 : save the 69 tif successfully for the year 2003 \n",
      "69 : save the 70 tif successfully for the year 2003 \n",
      "70 : save the 71 tif successfully for the year 2003 \n",
      "71 : save the 72 tif successfully for the year 2003 \n",
      "72 : save the 73 tif successfully for the year 2003 \n",
      "73 : save the 74 tif successfully for the year 2003 \n",
      "74 : save the 75 tif successfully for the year 2003 \n",
      "75 : save the 76 tif successfully for the year 2003 \n",
      "76 : save the 77 tif successfully for the year 2003 \n",
      "77 : save the 78 tif successfully for the year 2003 \n",
      "78 : save the 79 tif successfully for the year 2003 \n",
      "79 : save the 80 tif successfully for the year 2003 \n",
      "80 : save the 81 tif successfully for the year 2003 \n",
      "81 : save the 82 tif successfully for the year 2003 \n",
      "82 : save the 83 tif successfully for the year 2003 \n",
      "83 : save the 84 tif successfully for the year 2003 \n",
      "84 : save the 85 tif successfully for the year 2003 \n",
      "85 : save the 86 tif successfully for the year 2003 \n",
      "86 : save the 87 tif successfully for the year 2003 \n",
      "87 : save the 88 tif successfully for the year 2003 \n",
      "88 : save the 89 tif successfully for the year 2003 \n",
      "89 : save the 90 tif successfully for the year 2003 \n",
      "90 : save the 91 tif successfully for the year 2003 \n",
      "91 : save the 92 tif successfully for the year 2003 \n",
      "92 : save the 93 tif successfully for the year 2003 \n",
      "93 : save the 94 tif successfully for the year 2003 \n",
      "94 : save the 95 tif successfully for the year 2003 \n",
      "95 : save the 96 tif successfully for the year 2003 \n",
      "96 : save the 97 tif successfully for the year 2003 \n",
      "97 : save the 98 tif successfully for the year 2003 \n",
      "98 : save the 99 tif successfully for the year 2003 \n",
      "99 : save the 100 tif successfully for the year 2003 \n",
      "100 : save the 101 tif successfully for the year 2003 \n",
      "101 : save the 102 tif successfully for the year 2003 \n",
      "102 : save the 103 tif successfully for the year 2003 \n",
      "103 : save the 104 tif successfully for the year 2003 \n",
      "104 : save the 105 tif successfully for the year 2003 \n",
      "105 : save the 106 tif successfully for the year 2003 \n",
      "106 : save the 107 tif successfully for the year 2003 \n",
      "107 : save the 108 tif successfully for the year 2003 \n",
      "108 : save the 109 tif successfully for the year 2003 \n",
      "109 : save the 110 tif successfully for the year 2003 \n",
      "110 : save the 111 tif successfully for the year 2003 \n",
      "111 : save the 112 tif successfully for the year 2003 \n",
      "112 : save the 113 tif successfully for the year 2003 \n",
      "113 : save the 114 tif successfully for the year 2003 \n",
      "114 : save the 115 tif successfully for the year 2003 \n",
      "115 : save the 116 tif successfully for the year 2003 \n",
      "116 : save the 117 tif successfully for the year 2003 \n",
      "117 : save the 118 tif successfully for the year 2003 \n",
      "118 : save the 119 tif successfully for the year 2003 \n",
      "119 : save the 120 tif successfully for the year 2003 \n",
      "120 : save the 121 tif successfully for the year 2003 \n",
      "121 : save the 122 tif successfully for the year 2003 \n",
      "122 : save the 123 tif successfully for the year 2003 \n",
      "123 : save the 124 tif successfully for the year 2003 \n",
      "124 : save the 125 tif successfully for the year 2003 \n",
      "125 : save the 126 tif successfully for the year 2003 \n",
      "126 : save the 127 tif successfully for the year 2003 \n",
      "127 : save the 128 tif successfully for the year 2003 \n",
      "128 : save the 129 tif successfully for the year 2003 \n",
      "129 : save the 130 tif successfully for the year 2003 \n",
      "130 : save the 131 tif successfully for the year 2003 \n",
      "131 : save the 132 tif successfully for the year 2003 \n",
      "132 : save the 133 tif successfully for the year 2003 \n",
      "133 : save the 134 tif successfully for the year 2003 \n",
      "134 : save the 135 tif successfully for the year 2003 \n",
      "135 : save the 136 tif successfully for the year 2003 \n",
      "136 : save the 137 tif successfully for the year 2003 \n",
      "137 : save the 138 tif successfully for the year 2003 \n",
      "138 : save the 139 tif successfully for the year 2003 \n",
      "139 : save the 140 tif successfully for the year 2003 \n",
      "140 : save the 141 tif successfully for the year 2003 \n",
      "141 : save the 142 tif successfully for the year 2003 \n",
      "142 : save the 143 tif successfully for the year 2003 \n",
      "143 : save the 144 tif successfully for the year 2003 \n",
      "144 : save the 145 tif successfully for the year 2003 \n",
      "145 : save the 146 tif successfully for the year 2003 \n",
      "146 : save the 147 tif successfully for the year 2003 \n",
      "147 : save the 148 tif successfully for the year 2003 \n",
      "148 : save the 149 tif successfully for the year 2003 \n",
      "149 : save the 150 tif successfully for the year 2003 \n",
      "150 : save the 151 tif successfully for the year 2003 \n",
      "151 : save the 152 tif successfully for the year 2003 \n",
      "152 : save the 153 tif successfully for the year 2003 \n",
      "153 : save the 154 tif successfully for the year 2003 \n",
      "154 : save the 155 tif successfully for the year 2003 \n",
      "155 : save the 156 tif successfully for the year 2003 \n",
      "156 : save the 157 tif successfully for the year 2003 \n",
      "157 : save the 158 tif successfully for the year 2003 \n",
      "158 : save the 159 tif successfully for the year 2003 \n",
      "159 : save the 160 tif successfully for the year 2003 \n",
      "160 : save the 161 tif successfully for the year 2003 \n",
      "161 : save the 162 tif successfully for the year 2003 \n",
      "162 : save the 163 tif successfully for the year 2003 \n",
      "163 : save the 164 tif successfully for the year 2003 \n",
      "164 : save the 165 tif successfully for the year 2003 \n",
      "165 : save the 166 tif successfully for the year 2003 \n",
      "166 : save the 167 tif successfully for the year 2003 \n",
      "167 : save the 168 tif successfully for the year 2003 \n",
      "168 : save the 169 tif successfully for the year 2003 \n",
      "169 : save the 170 tif successfully for the year 2003 \n",
      "170 : save the 171 tif successfully for the year 2003 \n",
      "171 : save the 172 tif successfully for the year 2003 \n",
      "172 : save the 173 tif successfully for the year 2003 \n",
      "173 : save the 174 tif successfully for the year 2003 \n",
      "174 : save the 175 tif successfully for the year 2003 \n",
      "175 : save the 176 tif successfully for the year 2003 \n",
      "176 : save the 177 tif successfully for the year 2003 \n",
      "177 : save the 178 tif successfully for the year 2003 \n",
      "failed at index 179\n",
      "2004 ************************************************\n",
      "load model\n",
      "D:/SGYL/SM_results_data/check_points/Full_ResNet/Full_ResNet_2004_best.hdf5\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "load training data\n",
      "['NDVI', 'EVI', 'LST', 'LST_Diff', 'Pre', 'SWCI', 'VSDI', 'SIWSI', 'ET', 'TWI', 'DEM', 'Aspect', 'Slope', 'Clay', 'Sand', 'Silt', 'Lon', 'Lat', 'DOY']\n",
      "prepare file path\n",
      "start predict ************************************************\n",
      "0 : save the 1 tif successfully for the year 2004 \n",
      "failed at index 2\n",
      "2005 ************************************************\n",
      "load model\n",
      "D:/SGYL/SM_results_data/check_points/Full_ResNet/Full_ResNet_2005_best.hdf5\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "load training data\n",
      "['NDVI', 'EVI', 'LST', 'LST_Diff', 'Pre', 'SWCI', 'VSDI', 'SIWSI', 'ET', 'TWI', 'DEM', 'Aspect', 'Slope', 'Clay', 'Sand', 'Silt', 'Lon', 'Lat', 'DOY']\n",
      "prepare file path\n",
      "start predict ************************************************\n",
      "0 : save the 1 tif successfully for the year 2005 \n",
      "1 : save the 2 tif successfully for the year 2005 \n",
      "2 : save the 3 tif successfully for the year 2005 \n",
      "3 : save the 4 tif successfully for the year 2005 \n",
      "4 : save the 5 tif successfully for the year 2005 \n",
      "5 : save the 6 tif successfully for the year 2005 \n",
      "6 : save the 7 tif successfully for the year 2005 \n",
      "7 : save the 8 tif successfully for the year 2005 \n",
      "8 : save the 9 tif successfully for the year 2005 \n",
      "9 : save the 10 tif successfully for the year 2005 \n",
      "10 : save the 11 tif successfully for the year 2005 \n",
      "11 : save the 12 tif successfully for the year 2005 \n",
      "12 : save the 13 tif successfully for the year 2005 \n",
      "13 : save the 14 tif successfully for the year 2005 \n",
      "14 : save the 15 tif successfully for the year 2005 \n",
      "15 : save the 16 tif successfully for the year 2005 \n",
      "16 : save the 17 tif successfully for the year 2005 \n",
      "17 : save the 18 tif successfully for the year 2005 \n",
      "18 : save the 19 tif successfully for the year 2005 \n",
      "19 : save the 20 tif successfully for the year 2005 \n",
      "20 : save the 21 tif successfully for the year 2005 \n",
      "21 : save the 22 tif successfully for the year 2005 \n",
      "22 : save the 23 tif successfully for the year 2005 \n"
     ]
    }
   ],
   "source": [
    "#['NDVI', 'EVI', 'LST', 'LST_Diff', 'Pre', 'SWCI', 'VSDI', 'SIWSI', 'ET', 'TWI', 'dem', 'aspect', 'slope', 'clay', 'sand', 'silt', 'Lon', 'Lat']\n",
    "error_index_year = []\n",
    "error_index_day = []\n",
    "batch_size = 4096\n",
    "for year in range(2003,2006):\n",
    "    try:\n",
    "        print(year,'************************************************')\n",
    "        print('load model')\n",
    "        model_path = os.path.join('D:/SGYL/SM_results_data/check_points/Full_ResNet/','Full_ResNet_'+str(year)+'_best.hdf5')\n",
    "        #model_path = os.path.join('D:/SGYL/SM_results_data/model/Full_ResNet/','Full_ResNet_'+str(year)+'.hdf5')\n",
    "        print(model_path)\n",
    "        model = keras.models.load_model(model_path,custom_objects={'R2':R2})\n",
    "\n",
    "        print('load training data')\n",
    "        #load training data\n",
    "        data_train = pd.read_csv(os.path.join('D:/SGYL/SM_Downscaling_data/Train/split/','train_data_'+str(year)+'.csv'))\n",
    "        X_train = data_train.drop(['SM'],axis = 1)\n",
    "        X_train_columns = X_train.columns.to_list()\n",
    "        print(X_train.columns.to_list())\n",
    "        standarder = StandardScaler()\n",
    "        X_train = standarder.fit_transform(X_train)\n",
    "\n",
    "        print('prepare file path')\n",
    "        # prepare data path\n",
    "        feature_order = ['NDVI','EVI','LST','LST_Diff','Pre','SWCI','VSDI','SIWSI','ET'] #\n",
    "        static_feature = ['TWI','dem','aspect','slope','clay','sand','silt','Lon','Lat']\n",
    "\n",
    "        root = 'F:/SGYL/SM_results_data/DATA/data_'+str(year)+'/'\n",
    "\n",
    "        predictor_path = dict()\n",
    "        for dir in feature_order:\n",
    "            path = os.path.join(root,dir)\n",
    "            if dir == 'ET':\n",
    "                files = [os.path.join(path+'/',i) for i in os.listdir(path+'/')]\n",
    "            elif year in [2004,2008,2012,2016,2020]:\n",
    "                files = [os.path.join(path+'/',dir+'_'+str(year)+'_'+str(i)+'.tif') for i in range(1,367)]\n",
    "            else:\n",
    "                files = [os.path.join(path+'/',dir+'_'+str(year)+'_'+str(i)+'.tif') for i in range(1,366)]\n",
    "            predictor_path[dir] = files\n",
    "\n",
    "        path_length = len(predictor_path['LST'])\n",
    "\n",
    "        for var in static_feature:\n",
    "            path_i = os.path.join('F:/SGYL/SM_results_data/Static/',var+'.tif')\n",
    "            files = [path_i for i in range(path_length)]\n",
    "            predictor_path[var] = files\n",
    "\n",
    "        predictor_path = pd.DataFrame(predictor_path)\n",
    "        predictor_path.columns = feature_order+static_feature\n",
    "        \n",
    "        print('start predict','************************************************')       \n",
    "        \n",
    "        for i in range(predictor_path.shape[0]):#predictor_path.shape[0]\n",
    "            try:\n",
    "                #print(i,'************************************************')\n",
    "                data_i = ReadData(predictor_path.iloc[i:(i+1),:])\n",
    "                lon_lat = data_i[['Lon','Lat']]\n",
    "                data_i['DOY'] = int(i+1)\n",
    "                data_i.columns = X_train_columns\n",
    "                data_i = data_i.to_numpy()\n",
    "                data_i[data_i == noData] = np.nan\n",
    "                    #print(data_i.shape)\n",
    "                    #dealing with missing values median\n",
    "                for index in range(data_i.shape[1]):\n",
    "                    median_i = np.nanmedian(data_i[:,index])\n",
    "                    data_i[:,index][np.isnan(data_i[:,index])] = median_i\n",
    "                data_i = pd.DataFrame(data_i,columns=X_train_columns)\n",
    "                data_i = standarder.transform(data_i)\n",
    "                    # XGBoost 不需要reshape\n",
    "                data_i = data_i.reshape(data_i.shape[0],data_i.shape[1],1)\n",
    "                #print(data_i.shape)\n",
    "                data_ii = [data_i[i*batch_size:(i+1)*batch_size,:,:] for i in range(data_i.shape[0]//batch_size+1)]\n",
    "                data_pred = np.concatenate([model(i,training = False).numpy() for i in data_ii])#model.predict(data_i,batch_size=4096)\n",
    "                #print(data_pred.shape)\n",
    "                predicted_arr = Array2Image(data_pred,lon_lat['Lat'],lon_lat['Lon'])\n",
    "                save_path = 'F:/SGYL/SM_results_data/results/'+str(year)+'/'+'Full_ResNet'+'/'+'ResNet_'+str(year)+'_'+str(i+1)+'.tif'\n",
    "                predicted_arr = predicted_arr * 10000\n",
    "                convert2Tif(predicted_arr,save_path)\n",
    "                print(i,': save the %d tif successfully for the year %d '%(i+1,year))\n",
    "            except Exception as e:\n",
    "                print('failed at year index %d'%(year,i+1)) # 这里的index是第几天\n",
    "                print(repr(e))\n",
    "                error_index_day.append((year,i))    \n",
    "            continue\n",
    "    except:\n",
    "        print('failed at index %d'%(i+1)) # 这里的index是第几天\n",
    "        error_index_year.append((year,i))\n",
    "    continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(error_index_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (637490, 19, 1)\n",
      "X_validate shape: (191247, 19, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "year = 2017\n",
    "data_train = pd.read_csv(os.path.join('D:/SGYL/SM_results_data/Train/split/','train_data_'+str(year)+'.csv'))\n",
    "data_validate = pd.read_csv(os.path.join('D:/SGYL/SM_results_data/Train/split/','validate_data_'+str(year)+'.csv'))\n",
    "data_test = pd.read_csv(os.path.join('D:/SGYL/SM_results_data/Train/split/','test_data_'+str(year)+'.csv'))\n",
    "\n",
    "X_train = data_train.drop(['SM'],axis = 1)\n",
    "y_train = data_train['SM'].copy()   \n",
    "X_validate = data_validate.drop(['SM'],axis = 1)\n",
    "y_validate = data_validate['SM'].copy()\n",
    "X_test = data_test.drop(['SM'],axis = 1)\n",
    "y_test = data_test['SM'].copy()\n",
    "\n",
    "standarder = StandardScaler()\n",
    "X_train = standarder.fit_transform(X_train)\n",
    "X_validate = standarder.transform(X_validate)\n",
    "X_test = standarder.transform(X_test)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],1)\n",
    "X_validate = X_validate.reshape(X_validate.shape[0],X_validate.shape[1],1)\n",
    "X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n",
    "\n",
    "print('X_train shape:',X_train.shape)\n",
    "print('X_validate shape:',X_validate.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/SGYL/SM_results_data/check_points/Full_ResNet/Full_ResNet_2017_best.hdf5\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join('D:/SGYL/SM_results_data/check_points/Full_ResNet/','Full_ResNet_'+str(year)+'_best.hdf5')\n",
    "#model_path = os.path.join('D:/SGYL/SM_results_data/model/Full_ResNet/','Full_ResNet_'+str(year)+'.hdf5')\n",
    "print(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(model_path,custom_objects={'R2':R2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2562/2562 [==============================] - 8s 3ms/step - loss: 8.0081e-04 - R2: 0.9001 - mean_absolute_error: 0.0208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0008008122094906867, 0.900081217288971, 0.020775100216269493]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12f4ae37f749e26047b6c90ec13e3c0736099234c7c5f03a9452dabd216507b1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('sgyl_py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86e2ddda5f68ae1c79ac72022cc8c2d7af6bbc181db9ebedd4d8f03368fea52b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
