{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "from osgeo import osr\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_visible_devices(devices=physical_devices[0], device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#自定义matries 计算r2\n",
    "def R2(y_true, y_pred):\n",
    "    sst = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "    ssr = K.sum(K.square(y_pred - y_true))\n",
    "    R2 = 1 - ssr/sst\n",
    "    return R2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Array2Image(data,lats,lons):\n",
    "  # get the unique coordinates\n",
    "  uniqueLats = np.unique(lats)\n",
    "  uniqueLons = np.unique(lons)\n",
    " \n",
    "  # get number of columns and rows from coordinates\n",
    "  ncols = len(uniqueLons)\n",
    "  nrows = len(uniqueLats)\n",
    " \n",
    "  # determine pixelsizes\n",
    "  ys = uniqueLats[1] - uniqueLats[0]  \n",
    "  xs = uniqueLons[1] - uniqueLons[0]\n",
    " \n",
    "  # create an array with dimensions of image\n",
    "  # 这里把缺失值设置成了0，之后需要修改\n",
    "  arr = np.zeros([nrows, ncols], np.float32) #-9999\n",
    "  arr.fill(np.nan)\n",
    " \n",
    "  # fill the array with values\n",
    "  for counter in range(0,len(data),1):\n",
    "    index_lon = np.where(uniqueLons == lons[counter])[0][0]\n",
    "    index_lat = np.where(uniqueLats == lats[counter])[0][0]\n",
    "    arr[len(uniqueLats)-1-index_lat,index_lon] = data[counter]\n",
    "  return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadData(df:pd.DataFrame):\n",
    "    allData = []\n",
    "    for j in range(df.shape[1]):\n",
    "        data = []\n",
    "        for i in range(df.shape[0]):\n",
    "            #print(df.iloc[i,j])\n",
    "            data_i = gdal.Open(df.iloc[i,j])\n",
    "            band_i = data_i.GetRasterBand(1)\n",
    "            array_i = band_i.ReadAsArray()\n",
    "            #array_i[array_i == noData] = np.nan\n",
    "            data.append(np.reshape(array_i,(-1)))\n",
    "        allData.append(data)\n",
    "    allData = np.array(allData,dtype = np.float64).squeeze()\n",
    "    return pd.DataFrame(allData.T,columns=df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "templete  = gdal.Open('F:/SGYL/SM_Downscaling2/Lon.tif')\n",
    "def convert2Tif(data,outputPath):\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    tif = driver.Create(outputPath,data.shape[1],data.shape[0],1,gdal.GDT_Int16)\n",
    "    #geotransform = (73.49117339381806,0.008983152841195215,0.0,39.831299697859585,0.0,-0.008983152841195215)\n",
    "    tif.SetGeoTransform(templete.GetGeoTransform())\n",
    "    tif.SetProjection(templete.GetProjection())\n",
    "    tif.GetRasterBand(1).WriteArray(data)\n",
    "    #tif.GetRasterBand(1).SetNoDataValue()\n",
    "    tif.FlushCache()\n",
    "    del tif\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.4e+38\n"
     ]
    }
   ],
   "source": [
    "noData_i = gdal.Open('F:/SGYL/SM_results_data/DATA/data_2020/LST_Diff/LST_Diff_2020_1.tif')\n",
    "noData_band = noData_i.GetRasterBand(1)\n",
    "noData_arr = noData_band.ReadAsArray()\n",
    "noData = np.min(noData_arr)\n",
    "print(noData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001 ************************************************\n",
      "load model\n",
      "D:/SGYL/SM_results_data/check_points/CNN/CNN_2001_best.hdf5\n",
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "load training data\n",
      "['NDVI', 'EVI', 'LST', 'LST_Diff', 'Pre', 'SWCI', 'VSDI', 'SIWSI', 'ET', 'TWI', 'DEM', 'Slope', 'Clay', 'Sand', 'Silt', 'Lon', 'Lat', 'DOY']\n",
      "prepare file path\n",
      "['NDVI', 'EVI', 'LST', 'LST_Diff', 'Pre', 'SWCI', 'VSDI', 'SIWSI', 'ET', 'TWI', 'dem', 'slope', 'clay', 'sand', 'silt', 'Lon', 'Lat']\n",
      "start predict ************************************************\n",
      "294 : save the 295 tif successfully for the year 2001 \n",
      "295 : save the 296 tif successfully for the year 2001 \n",
      "296 : save the 297 tif successfully for the year 2001 \n",
      "297 : save the 298 tif successfully for the year 2001 \n",
      "298 : save the 299 tif successfully for the year 2001 \n",
      "299 : save the 300 tif successfully for the year 2001 \n",
      "300 : save the 301 tif successfully for the year 2001 \n",
      "301 : save the 302 tif successfully for the year 2001 \n",
      "302 : save the 303 tif successfully for the year 2001 \n",
      "303 : save the 304 tif successfully for the year 2001 \n",
      "304 : save the 305 tif successfully for the year 2001 \n",
      "305 : save the 306 tif successfully for the year 2001 \n",
      "306 : save the 307 tif successfully for the year 2001 \n",
      "307 : save the 308 tif successfully for the year 2001 \n",
      "308 : save the 309 tif successfully for the year 2001 \n",
      "309 : save the 310 tif successfully for the year 2001 \n",
      "310 : save the 311 tif successfully for the year 2001 \n",
      "311 : save the 312 tif successfully for the year 2001 \n",
      "312 : save the 313 tif successfully for the year 2001 \n",
      "313 : save the 314 tif successfully for the year 2001 \n",
      "314 : save the 315 tif successfully for the year 2001 \n",
      "315 : save the 316 tif successfully for the year 2001 \n",
      "316 : save the 317 tif successfully for the year 2001 \n",
      "317 : save the 318 tif successfully for the year 2001 \n",
      "318 : save the 319 tif successfully for the year 2001 \n",
      "319 : save the 320 tif successfully for the year 2001 \n",
      "320 : save the 321 tif successfully for the year 2001 \n",
      "321 : save the 322 tif successfully for the year 2001 \n",
      "322 : save the 323 tif successfully for the year 2001 \n",
      "323 : save the 324 tif successfully for the year 2001 \n",
      "324 : save the 325 tif successfully for the year 2001 \n",
      "325 : save the 326 tif successfully for the year 2001 \n",
      "326 : save the 327 tif successfully for the year 2001 \n",
      "327 : save the 328 tif successfully for the year 2001 \n",
      "328 : save the 329 tif successfully for the year 2001 \n",
      "329 : save the 330 tif successfully for the year 2001 \n",
      "330 : save the 331 tif successfully for the year 2001 \n",
      "331 : save the 332 tif successfully for the year 2001 \n",
      "332 : save the 333 tif successfully for the year 2001 \n",
      "333 : save the 334 tif successfully for the year 2001 \n",
      "334 : save the 335 tif successfully for the year 2001 \n",
      "335 : save the 336 tif successfully for the year 2001 \n",
      "336 : save the 337 tif successfully for the year 2001 \n",
      "337 : save the 338 tif successfully for the year 2001 \n",
      "338 : save the 339 tif successfully for the year 2001 \n",
      "339 : save the 340 tif successfully for the year 2001 \n",
      "340 : save the 341 tif successfully for the year 2001 \n",
      "341 : save the 342 tif successfully for the year 2001 \n",
      "342 : save the 343 tif successfully for the year 2001 \n",
      "343 : save the 344 tif successfully for the year 2001 \n",
      "344 : save the 345 tif successfully for the year 2001 \n",
      "345 : save the 346 tif successfully for the year 2001 \n",
      "346 : save the 347 tif successfully for the year 2001 \n",
      "347 : save the 348 tif successfully for the year 2001 \n",
      "348 : save the 349 tif successfully for the year 2001 \n",
      "349 : save the 350 tif successfully for the year 2001 \n",
      "350 : save the 351 tif successfully for the year 2001 \n",
      "351 : save the 352 tif successfully for the year 2001 \n",
      "352 : save the 353 tif successfully for the year 2001 \n",
      "353 : save the 354 tif successfully for the year 2001 \n",
      "354 : save the 355 tif successfully for the year 2001 \n",
      "355 : save the 356 tif successfully for the year 2001 \n",
      "356 : save the 357 tif successfully for the year 2001 \n",
      "357 : save the 358 tif successfully for the year 2001 \n",
      "358 : save the 359 tif successfully for the year 2001 \n",
      "359 : save the 360 tif successfully for the year 2001 \n",
      "360 : save the 361 tif successfully for the year 2001 \n"
     ]
    }
   ],
   "source": [
    "#['NDVI', 'EVI', 'LST', 'LST_Diff', 'Pre', 'SWCI', 'VSDI', 'SIWSI', 'ET', 'TWI', 'dem', 'aspect', 'slope', 'clay', 'sand', 'silt', 'Lon', 'Lat']\n",
    "batch_size = 4096*4\n",
    "error_index_year = []\n",
    "error_index_day = []\n",
    "for year in range(2001,2002):\n",
    "    try:\n",
    "        print(year,'************************************************')\n",
    "        print('load model')\n",
    "        model_path = os.path.join('D:/SGYL/SM_results_data/check_points/CNN/','CNN_'+str(year)+'_best.hdf5')\n",
    "        print(model_path)\n",
    "        model = keras.models.load_model(model_path,custom_objects={'R2':R2})\n",
    "\n",
    "        print('load training data')\n",
    "        #load training data\n",
    "        data_train = pd.read_csv(os.path.join('D:/SGYL/SM_Downscaling_data/Train/split/','train_data_'+str(year)+'.csv'))\n",
    "        X_train = data_train.drop(['SM','Aspect'],axis = 1)\n",
    "        X_train_columns = X_train.columns.to_list()\n",
    "        print(X_train.columns.to_list())\n",
    "        standarder = StandardScaler()\n",
    "        X_train = standarder.fit_transform(X_train)\n",
    "\n",
    "        print('prepare file path')\n",
    "        # prepare data path\n",
    "        feature_order = ['NDVI','EVI','LST','LST_Diff','Pre','SWCI','VSDI','SIWSI','ET'] #\n",
    "        static_feature = ['TWI','dem','slope','clay','sand','silt','Lon','Lat']\n",
    "        print(feature_order+static_feature)\n",
    "        root = 'F:/SGYL/SM_results_data/DATA/data_'+str(year)+'/'\n",
    "\n",
    "        predictor_path = dict()\n",
    "        for dir in feature_order:\n",
    "            path = os.path.join(root,dir)\n",
    "            if dir == 'ET':\n",
    "                files = [os.path.join(path+'/',i) for i in os.listdir(path+'/')]\n",
    "            elif year in [2004,2008,2012,2016,2020]:\n",
    "                files = [os.path.join(path+'/',dir+'_'+str(year)+'_'+str(i)+'.tif') for i in range(1,367)]\n",
    "            elif year == 2001:\n",
    "                files = [os.path.join(path+'/',dir+'_'+str(year)+'_'+str(i)+'.tif') for i in range(1,362)]\n",
    "            else:\n",
    "                files = [os.path.join(path+'/',dir+'_'+str(year)+'_'+str(i)+'.tif') for i in range(1,366)]\n",
    "            predictor_path[dir] = files\n",
    "\n",
    "        path_length = len(predictor_path['LST'])\n",
    "\n",
    "        for var in static_feature:\n",
    "            path_i = os.path.join('F:/SGYL/SM_results_data/Static/',var+'.tif')\n",
    "            files = [path_i for i in range(path_length)]\n",
    "            predictor_path[var] = files\n",
    "\n",
    "        predictor_path = pd.DataFrame(predictor_path)\n",
    "        predictor_path.columns = feature_order+static_feature\n",
    "        \n",
    "        print('start predict','************************************************')       \n",
    "        \n",
    "        for i in range(294,predictor_path.shape[0]):#predictor_path.shape[0]\n",
    "            try:\n",
    "                #print(i,'************************************************')\n",
    "                data_i = ReadData(predictor_path.iloc[i:(i+1),:])\n",
    "                lon_lat = data_i[['Lon','Lat']]\n",
    "                data_i['DOY'] = int(i+1)\n",
    "                data_i.columns = X_train_columns\n",
    "                data_i = data_i.to_numpy()\n",
    "                data_i[data_i == noData] = np.nan\n",
    "                    #print(data_i.shape)\n",
    "                    #dealing with missing values median\n",
    "                for index in range(data_i.shape[1]):\n",
    "                    median_i = np.nanmedian(data_i[:,index])\n",
    "                    data_i[:,index][np.isnan(data_i[:,index])] = median_i\n",
    "                data_i = pd.DataFrame(data_i,columns=X_train_columns)\n",
    "                data_i = standarder.transform(data_i)\n",
    "                    # XGBoost 不需要reshape\n",
    "                data_i = data_i.reshape(data_i.shape[0],data_i.shape[1],1)\n",
    "                #print(data_i.shape)\n",
    "                data_ii = [data_i[i*batch_size:(i+1)*batch_size,:,:] for i in range(data_i.shape[0]//batch_size+1)]\n",
    "                data_pred = np.concatenate([model(i,training = False).numpy() for i in data_ii])#model.predict(data_i,batch_size=4096)\n",
    "                #print(data_pred.shape)\n",
    "                predicted_arr = Array2Image(data_pred,lon_lat['Lat'],lon_lat['Lon'])\n",
    "                save_path = 'F:/SGYL/SM_results_data/results/'+str(year)+'/'+'CNN'+'/'+'CNN_'+str(year)+'_'+str(i+1)+'.tif'\n",
    "                predicted_arr = predicted_arr * 10000\n",
    "                convert2Tif(predicted_arr,save_path)\n",
    "                print(i,': save the %d tif successfully for the year %d '%(i+1,year))\n",
    "            except Exception as e:\n",
    "                print('failed at year %d index %d'%(year,i+1)) # 这里的index是第几天\n",
    "                print(repr(e))\n",
    "                error_index_day.append((year,i))\n",
    "            continue\n",
    "    except:\n",
    "        print('failed at index %d'%(i+1)) # 这里的index是第几天\n",
    "        error_index_year.append((year,i))\n",
    "    continue\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#print(error_index_year)\n",
    "print(error_index_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#['NDVI', 'EVI', 'LST', 'LST_Diff', 'Pre', 'SWCI', 'VSDI', 'SIWSI', 'ET', 'TWI', 'dem', 'aspect', 'slope', 'clay', 'sand', 'silt', 'Lon', 'Lat']\n",
    "error_index_year = []\n",
    "error_index_day = []\n",
    "for year in [2013]:\n",
    "    try:\n",
    "        print(year,'************************************************')\n",
    "        print('load model')\n",
    "        model_path = os.path.join('D:/SGYL/SM_results_data/check_points/CNN/','CNN_'+str(year)+'_best.hdf5')\n",
    "        print(model_path)\n",
    "        model = keras.models.load_model(model_path,custom_objects={'R2':R2})\n",
    "\n",
    "        print('load training data')\n",
    "        #load training data\n",
    "        data_train = pd.read_csv(os.path.join('D:/SGYL/SM_Downscaling_data/Train/split/','train_data_'+str(year)+'.csv'))\n",
    "        X_train = data_train.drop(['SM','Aspect'],axis = 1)\n",
    "        X_train_columns = X_train.columns.to_list()\n",
    "        print(X_train.columns.to_list())\n",
    "        standarder = StandardScaler()\n",
    "        X_train = standarder.fit_transform(X_train)\n",
    "\n",
    "        print('prepare file path')\n",
    "        # prepare data path\n",
    "        feature_order = ['NDVI','EVI','LST','LST_Diff','Pre','SWCI','VSDI','SIWSI','ET'] #\n",
    "        static_feature = ['TWI','dem','slope','clay','sand','silt','Lon','Lat']\n",
    "        print(feature_order+static_feature)\n",
    "        root = 'F:/SGYL/SM_results_data/DATA/data_'+str(year)+'/'\n",
    "\n",
    "        predictor_path = dict()\n",
    "        for dir in feature_order:\n",
    "            path = os.path.join(root,dir)\n",
    "            if dir == 'ET':\n",
    "                files = [os.path.join(path+'/',i) for i in os.listdir(path+'/')]\n",
    "            elif year in [2004,2008,2012,2016,2020]:\n",
    "                files = [os.path.join(path+'/',dir+'_'+str(year)+'_'+str(i)+'.tif') for i in range(1,367)]\n",
    "            else:\n",
    "                files = [os.path.join(path+'/',dir+'_'+str(year)+'_'+str(i)+'.tif') for i in range(1,366)]\n",
    "            predictor_path[dir] = files\n",
    "\n",
    "        path_length = len(predictor_path['LST'])\n",
    "\n",
    "        for var in static_feature:\n",
    "            path_i = os.path.join('F:/SGYL/SM_results_data/Static/',var+'.tif')\n",
    "            files = [path_i for i in range(path_length)]\n",
    "            predictor_path[var] = files\n",
    "\n",
    "        predictor_path = pd.DataFrame(predictor_path)\n",
    "        predictor_path.columns = feature_order+static_feature\n",
    "        \n",
    "        print('start predict','************************************************')       \n",
    "        \n",
    "        for i in range(predictor_path.shape[0]):#predictor_path.shape[0]\n",
    "            try:\n",
    "                #print(i,'************************************************')\n",
    "                data_i = ReadData(predictor_path.iloc[i:(i+1),:])\n",
    "                lon_lat = data_i[['Lon','Lat']]\n",
    "                data_i['DOY'] = int(i+1)\n",
    "                data_i.columns = X_train_columns\n",
    "                data_i = data_i.to_numpy()\n",
    "                data_i[data_i == noData] = np.nan\n",
    "                    #print(data_i.shape)\n",
    "                    #dealing with missing values median\n",
    "                for index in range(data_i.shape[1]):\n",
    "                    median_i = np.nanmedian(data_i[:,index])\n",
    "                    data_i[:,index][np.isnan(data_i[:,index])] = median_i\n",
    "                data_i = pd.DataFrame(data_i,columns=X_train_columns)\n",
    "                data_i = standarder.transform(data_i)\n",
    "                    # XGBoost 不需要reshape\n",
    "                data_i = data_i.reshape(data_i.shape[0],data_i.shape[1],1)\n",
    "                data_pred = model.predict(data_i,batch_size=4096*4)\n",
    "                #print(data_pred.shape)\n",
    "                predicted_arr = Array2Image(data_pred,lon_lat['Lat'],lon_lat['Lon'])\n",
    "                save_path = 'F:/SGYL/SM_results_data/results/'+str(year)+'/'+'CNN'+'/'+'CNN_'+str(year)+'_'+str(i+1)+'.tif'\n",
    "                predicted_arr = predicted_arr * 10000\n",
    "                convert2Tif(predicted_arr,save_path)\n",
    "                print(i,': save the %d tif successfully for the year %d '%(i+1,year))\n",
    "            except Exception as e:\n",
    "                print('failed at year %d index %d'%(year,i+1)) # 这里的index是第几天\n",
    "                print(repr(e))\n",
    "                error_index_day.append((year,i))\n",
    "            continue\n",
    "    except:\n",
    "        print('failed at index %d'%(i+1)) # 这里的index是第几天\n",
    "        error_index_year.append((year,i))\n",
    "    continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error_index_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = gdal.Open('E:/SM_results_2/25km_ESACCI/allyear/clipped/LST/LST_D/LST_D_final_2001_2001.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = []\n",
    "for i in range(test.RasterCount):\n",
    "    band_i = test.GetRasterBand((i+1))\n",
    "    array_i = band_i.ReadAsArray()\n",
    "    alldata.append(array_i)\n",
    "alldata = np.array(alldata,dtype= np.float64)\n",
    "alldata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.isnan(alldata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12f4ae37f749e26047b6c90ec13e3c0736099234c7c5f03a9452dabd216507b1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('sgyl_py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86e2ddda5f68ae1c79ac72022cc8c2d7af6bbc181db9ebedd4d8f03368fea52b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
